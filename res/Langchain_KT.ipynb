{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c511d0-b497-4c3d-abbb-7c0d37d76e30",
   "metadata": {},
   "source": [
    "## Environment setup & installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecc89a-5bf3-4c57-8276-6f801b1fad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conda environemnt\n",
    "# conda create langchain python==3.10\n",
    "# conda activate langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857b191-6114-4126-8412-e9b0173a6ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Langchain Installation\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602e003-3a42-44d9-a176-83d8e1b8bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac19e19-50b0-40e1-90af-dbc405b26301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55609253-15cb-4e8c-8c34-a1d8f1532756",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29b8a6-cd05-4788-937b-751a53729400",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ca850-d0d5-49eb-a84a-7555fd5e5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81d3fea9-53ac-4e33-a5db-9856a94585ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numexpr\n",
      "  Downloading numexpr-2.8.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/sarathchandra/miniconda3/envs/langchain/lib/python3.10/site-packages (from numexpr) (1.26.3)\n",
      "Downloading numexpr-2.8.8-cp310-cp310-macosx_11_0_arm64.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numexpr\n",
      "Successfully installed numexpr-2.8.8\n"
     ]
    }
   ],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a52be-afc0-4fe9-86a7-3716d5f49256",
   "metadata": {},
   "source": [
    "# LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ce327a7-b66d-48cd-8bb0-6b0333565f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set your keys here!\n",
    "os.environ['OPENAI_API_KEY']=\"Give Your API Key\"\n",
    "# Get your key from here: https://tavily.com/ (Used for Agents)\n",
    "# Tavily’s Search API is a search engine built specifically for AI agents (LLMs), \n",
    "# delivering real-time, accurate, and factual results at speed.\n",
    "os.environ['TAVILY_API_KEY'] = 'Give Your TAVILY_API_KEY Key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca172350-a5c1-4af5-b8b4-54a3172ec12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using OpenAI (a popular model available via API)\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "633c1fd2-6705-42d3-a1c6-844cf12ae378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith is not a widely known term or concept. It does not appear to have a specific definition or meaning. It is possible that it could be a proper noun, such as a name of a person or a company, but without further context, it is difficult to determine its specific reference.')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is langsmith?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4d0233e-a133-47bb-b4fc-88f742b2bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can guide it's response with a prompt template. \n",
    "# Prompt templates are used to convert raw user input to a better input to the LLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer. Be as true as possible\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bb7f98f5-4225-45f7-8f0b-b6edf26685ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine these into a simple LLM chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cbc707c-89bd-422d-b236-9f0164e7672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangSmith is a powerful programming language designed to be simple, efficient, and versatile. It was developed with the goal of providing programmers with a comprehensive set of tools and features to facilitate the development of robust and scalable software applications.\\n\\nOne of the key features of LangSmith is its expressive syntax, which allows developers to write clean and readable code. The language supports a wide range of programming paradigms, including procedural, object-oriented, and functional programming, making it suitable for various application domains.\\n\\nLangSmith provides an extensive standard library, offering a rich set of functions and modules that can be used to streamline development tasks. It also supports seamless integration with other programming languages, allowing developers to leverage existing code and libraries.\\n\\nThe language is known for its powerful error handling mechanisms, which enable developers to write robust and reliable software. It provides comprehensive debugging features, including a built-in debugger and profiling tools, that aid in identifying and resolving issues efficiently.\\n\\nAdditionally, LangSmith offers excellent performance, with a highly optimized runtime environment and efficient memory management. It supports concurrent and parallel programming, allowing developers to take advantage of modern multi-core processors and distributed computing environments.\\n\\nLangSmith is backed by an active and vibrant community of developers, who contribute to its growth and provide support through forums, documentation, and open-source libraries. The language continues to evolve with regular updates and improvements, ensuring that it remains relevant and up-to-date with the latest technological advancements.\\n\\nOverall, LangSmith empowers programmers to write high-quality software by providing a powerful and flexible toolset, making it an ideal choice for a wide range of software development projects.')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke it and ask the same question. \n",
    "# It still won't know the answer, but it should respond in a more proper tone for a technical writer!\n",
    "chain.invoke({\"input\": \"What is LangSmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "34dc64e7-9f49-4617-b855-be8d94a65522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple output parser to convert the chat message to a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad526c37-244b-4616-9a88-5524c86c4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f2584b24-aa55-420a-94e5-fc5db0f333fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a cutting-edge programming language that combines the power and flexibility of multiple programming paradigms to enable developers to write efficient and maintainable code. It is designed to meet the needs of modern software development by providing a rich set of features and tools for creating scalable and robust applications.\\n\\nWith LangSmith, developers can leverage object-oriented, functional, and procedural programming styles, allowing them to choose the approach that best suits their specific requirements. The language supports static typing, which helps catch errors early in the development process and improves code quality.\\n\\nOne of the key features of LangSmith is its extensive standard library, which provides a wide range of built-in functions and modules for common tasks such as file I/O, networking, and data manipulation. This library empowers developers to streamline their development process by reducing the need for external dependencies.\\n\\nLangSmith also offers advanced features like concurrency support, which allows developers to write concurrent and parallel code more easily, taking advantage of modern multi-core processors. Additionally, the language provides strong support for error handling and exception management, ensuring robustness and reliability in applications.\\n\\nTo enhance developer productivity, LangSmith provides a comprehensive integrated development environment (IDE) with features such as code autocompletion, syntax highlighting, and debugging tools. These features help streamline the development workflow and enable developers to write code more efficiently.\\n\\nOverall, LangSmith aims to provide a powerful and intuitive programming language that enables developers to write high-quality software with ease. Its combination of programming paradigms, extensive standard library, and developer-friendly tools make it a valuable choice for a wide range of application development tasks.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is LangSmith?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebeecd4-1515-4b16-ad99-d5e2e7f968e0",
   "metadata": {},
   "source": [
    "## Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f7b13027-807b-4fac-88d8-484ee9186c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0f3f75d2-d57d-43bd-930a-08de6f51708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "911b511d-ec7a-4453-a62e-d0a7baf1862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af71d209-98d2-45b0-b449-8ca299a26d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9edb1703-5660-4484-b439-436d79f00ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "09c692b2-c23f-4a87-a4a0-75a28c487886",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "619ae60d-c4d6-4c83-8225-c928600e10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a tool or platform used for building reliable LLM (Language Learning Model) applications. It provides features such as tracing, debugging, monitoring, testing, evaluation, and dataset exporting. It helps developers understand and optimize the performance of prompts, chains, and agents in LLM applications.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"What is LangSmith?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebea063-bdef-4d1f-ad0e-90afa0495306",
   "metadata": {},
   "source": [
    "## Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e0a1f42d-f68a-429a-b18d-cda6ae916429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for observability\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "set_verbose(True)\n",
    "# Set this to true to see how Conversation Retrieval Chain works!\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a71c9f24-6b82-406b-bfbd-d822482ad1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6b3e521f-c7e1-402c-8d34-9d376dd1cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5a7d3abb-f24d-483d-950a-57904da51b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "457f59d3-67e4-4d78-87d4-644e3ffd2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "65dfc66e-aad7-4b9f-9368-b63f74802695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': 'LangSmith provides several features that can help test your LLM applications:\\n\\n1. Dataset Testing: LangSmith simplifies dataset uploading, allowing you to test changes to prompts or chains using a dataset. You can run the chain over the data points and visualize the outputs. The LangSmith client makes it easy to pull down a dataset, run a chain over them, and log the results to a new project associated with the dataset. You can then review the results and assign feedback to runs.\\n\\n2. Evaluators: LangSmith includes a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While these evaluators may not be perfect, they can guide your attention to examples that require manual review, especially when dealing with a large number of data points.\\n\\n3. Human Evaluation: LangSmith makes it easy to manually review and annotate runs through annotation queues. These queues allow you to select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. As a reviewer, you can quickly step through the runs, view the input and output, and add your own feedback. This is useful for assessing subjective qualities like creativity or humor, as well as validating the results of auto-evaluation metrics.\\n\\nBy utilizing these features, LangSmith can assist you in testing and evaluating your LLM applications, ensuring their reliability and quality.'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd9d70-9e26-4648-9f50-ee6c89a4dd27",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "493ef2a5-02b5-49a9-87c8-f1eda0c4802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "64a38086-f0c1-4de7-a4a1-ee7140667a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f521c60-eed2-45d0-bdb9-6957ea125de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0233d8ce-388d-4411-bf36-a8372927028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b1baca8f-6632-45f5-8d43-f443bbea1757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help with testing in several ways:\n",
      "\n",
      "1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications or requirements of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different paths and conditions in the code.\n",
      "\n",
      "2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the code and specifications to automatically generate assertions or checks that can be used as test oracles.\n",
      "\n",
      "3. Test Coverage Analysis: LangSmith can analyze the code and track the coverage of the test cases. It can provide information on which parts of the code are covered by the tests and which parts are not. This helps in identifying areas that need more testing and improving the overall test coverage.\n",
      "\n",
      "4. Test Prioritization: LangSmith can prioritize the test cases based on different criteria such as code coverage, complexity, or criticality. This helps in optimizing the testing process by focusing on the most important or high-risk areas first.\n",
      "\n",
      "5. Test Execution: LangSmith can execute the generated test cases automatically and provide feedback on the results. It can detect failures or errors in the code and provide detailed information on the root cause of the failure.\n",
      "\n",
      "Overall, LangSmith can assist in automating and optimizing the testing process, making it more efficient and effective.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'LangSmith can help with testing in several ways:\\n\\n1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications or requirements of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different paths and conditions in the code.\\n\\n2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the code and specifications to automatically generate assertions or checks that can be used as test oracles.\\n\\n3. Test Coverage Analysis: LangSmith can analyze the code and track the coverage of the test cases. It can provide information on which parts of the code are covered by the tests and which parts are not. This helps in identifying areas that need more testing and improving the overall test coverage.\\n\\n4. Test Prioritization: LangSmith can prioritize the test cases based on different criteria such as code coverage, complexity, or criticality. This helps in optimizing the testing process by focusing on the most important or high-risk areas first.\\n\\n5. Test Execution: LangSmith can execute the generated test cases automatically and provide feedback on the results. It can detect failures or errors in the code and provide detailed information on the root cause of the failure.\\n\\nOverall, LangSmith can assist in automating and optimizing the testing process, making it more efficient and effective.'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "066c4879-8f2a-4e19-90e7-525ec1828255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'weather in Bangalore'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://weatherspark.com/h/m/108998/2024/1/Historical-Weather-in-January-2024-in-Bengaluru-India', 'content': 'January 2024 Weather History in Bengaluru India  Bengaluru Temperature History January 2024 Hourly Temperature in January 2024 in Bengaluru  Observed Weather in January 2024 in Bengaluru Hours of Daylight and Twilight in January 2024 in Bengaluru  Compare Bengaluru to another city: Cloud Cover in January 2024 in BengaluruPartly Cloudy 1,500 ft. This report shows the past weather for Bengaluru, providing a weather history for January 2024. It features all historical weather data series we have available, including the Bengaluru temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.'}, {'url': 'https://www.weather2travel.com/india/bangalore/january/', 'content': 'Bangalore weather in January 2024 Expect 27°C daytime maximum temperatures  weather averages for Bangalore in January before you book your next holiday to India in 2024/2025.  Bangalore January sunrise & sunset times  How sunny is it in Bangalore in January?27 27°C max day temperature 9 9 hours of sunshine per day 0 0 days with some rainfall 15 15°C min night temperature 11 11 hours of daylight per day L Low heat & humidity 3 3 mm of monthly rainfall 10 10 (Very High) UV index More about Bangalore Overview Weather by month 5-day weather forecast Destinations Best time to visit Travel advice'}, {'url': 'https://en.climate-data.org/asia/india/karnataka/bengaluru-4562/t/january-1/', 'content': 'Bengaluru Weather in January  all information about the weather in Bengaluru in January:  Bengaluru weather in January Bengaluru weather by month // weather averages 20.7 (69.3) 14.6 (58.4) 27.4 (81.2) 4 (0.2)  Bengaluru weather in January // weather averages Airport close to BengaluruData: 1991 - 2021 Min. Temperature °C (°F), Max. Temperature °C (°F), Precipitation / Rainfall mm (in), Humidity, Rainy days. Data: 1999 - 2019: avg. Sun hours Bengaluru weather and climate for further months Bengaluru in February Bengaluru in March Bengaluru in April Bengaluru in May Bengaluru in June Bengaluru in July Bengaluru in August'}, {'url': 'https://www.weather25.com/asia/india/karnataka/bangalore', 'content': \"The weather today in Bangalore Bangalore weather report  will give you an idea of weather trends in Bangalore. For example, the weather in Bangalore in December 2023.  The weather in Bangalore, India Bangalore weather by months Bangalore weather Bangalore 14 day weather  Weather25.com provides all the information that you need to know about the weather in Bangalore, India. Stay up to dateFor example, the weather in Bangalore in January 2024. These trends can be helpful when planning trips to Bangalore or preparing for the weather in advance. There are many factors to consider when looking at the weather in Bangalore. These factors include the time of year, season, the Bangalore's altitude, surrounding weather patterns and more.\"}]\u001b[0m\u001b[32;1m\u001b[1;3mI found some information about the weather in Bangalore. In January 2024, the average maximum temperature is expected to be around 27°C during the day, with a minimum night temperature of 15°C. There will be about 9 hours of sunshine per day and a low chance of rainfall, with an average of 3 mm of monthly rainfall. The weather is generally characterized as having low heat and humidity. Please note that this information is based on historical weather data and may vary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the weather in Bangalore?',\n",
       " 'output': 'I found some information about the weather in Bangalore. In January 2024, the average maximum temperature is expected to be around 27°C during the day, with a minimum night temperature of 15°C. There will be about 9 hours of sunshine per day and a low chance of rainfall, with an average of 3 mm of monthly rainfall. The weather is generally characterized as having low heat and humidity. Please note that this information is based on historical weather data and may vary.'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in Bangalore?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de767122-19f2-4559-8b71-8cfeda662e11",
   "metadata": {},
   "source": [
    "# ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6bf27386-4044-4460-b872-c505dc23b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bf23bfdb-5cec-493e-a63f-f52bae729e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain that interprets a prompt and executes python code to do math.\n",
    "llm_math = LLMMathChain.from_llm(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e92e9e97-c870-42ba-9019-f1134f0253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    \"\"\"A calculator for performing math.\"\"\"\n",
    "    return llm_math.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6ebec38e-89f2-4877-ad98-47e8615e0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily’s Search API is a search engine built specifically for AI agents (LLMs)\n",
    "search = TavilySearchResults(max_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b1fa4a27-94cf-438e-a6c2-9fa2ad970039",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search, calculator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b9349b7b-365b-433b-a30d-269e98699192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71a62e99-a7df-45ae-9c65-d8b8e128c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM to use\n",
    "llm = OpenAI()\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "10d3adde-3022-4907-9480-ff0994829693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d45ca494-ea24-4e3e-bd3a-52163684e8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use tavily_search_results_json to find the current weather in Bangalore\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"weather in Bangalore\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://weatherspark.com/h/m/108998/2024/1/Historical-Weather-in-January-2024-in-Bengaluru-India', 'content': 'January 2024 Weather History in Bengaluru India  Bengaluru Temperature History January 2024 Hourly Temperature in January 2024 in Bengaluru  Observed Weather in January 2024 in Bengaluru Hours of Daylight and Twilight in January 2024 in Bengaluru  Compare Bengaluru to another city: Cloud Cover in January 2024 in BengaluruPartly Cloudy 1,500 ft. This report shows the past weather for Bengaluru, providing a weather history for January 2024. It features all historical weather data series we have available, including the Bengaluru temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.'}]\u001b[0m\u001b[32;1m\u001b[1;3m I should use calculator to convert the temperature to Celsius\n",
      "Action: calculator\n",
      "Action Input: \"32F to C\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "32F to C\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: 32F to C\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```text\n",
      "(32 - 32) * (5/9)\n",
      "```\n",
      "\n",
      "...numexpr.evaluate(\"(32 - 32) * (5/9)\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m0.0\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{'question': '32F to C', 'answer': 'Answer: 0.0'}\u001b[0m\u001b[32;1m\u001b[1;3m0 degrees Celsius is the final answer\n",
      "Final Answer: 0 degrees Celsius\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the weather in Bangalore? Convert it to Celsius.',\n",
       " 'output': '0 degrees Celsius'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is the weather in Bangalore? Convert it to Celsius.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "98cb7aac-bd02-4ead-99a3-c566c70f19e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use tavily_search_results_json because I need accurate and trusted results for current events.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://people.com/music/harry-styles-olivia-wilde-relationship-timeline/', 'content': 'August 24, 2022: Olivia Wilde says she and Harry Styles are protective of their relationship  Harry Styles and Olivia Wilde\\'s Relationship Timeline  Styles and Olivia Wilde\\'s relationship.  April 2022: Harry Styles reveals he\\'s \"really happy\" amid his relationship with Olivia WildeNovember 18, 2022: Olivia Wilde and Harry Styles are \"taking a break\". After nearly two years together, Wilde and Styles are pressing pause on their romance. Multiple sources confirmed exclusively ...'}]\u001b[0m\u001b[32;1m\u001b[1;3mI should use calculator to calculate the current age raised to the 0.23 power.\n",
      "Action: calculator\n",
      "Action Input: 23^0.23\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "23^0.23\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: 23^0.23\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```text\n",
      "23**0.23\n",
      "```\n",
      "...numexpr.evaluate(\"23**0.23\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.0568252837687546\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{'question': '23^0.23', 'answer': 'Answer: 2.0568252837687546'}\u001b[0m\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 2.0568252837687546\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\",\n",
       " 'output': '2.0568252837687546'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a7a814b3-ad5f-4482-b8a9-ce026338f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m We need to find the cost of a flight from Bangalore to Hyderabad and then multiply it by a certain number.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"flight cost Bangalore to Hyderabad\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.easemytrip.com/flights/bangalore-blr-to-hyderabad-hyd/', 'content': 'Bangalore to Hyderabad Flights Details  Bangalore to Hyderabad Flight Information  HYD 09:00 Hyderabad Rajiv Gandhi International Airport Wed, 17 Jan 2024 Find the Lowest Airfare Bangalore to Hyderabad  Best Flight From Bangalore To Hyderabad BLR 07:25 Bangalore Kempegowda International Airport Wed , 17 Jan 2024 HYDReturn Date Book a round trip to save more TRAVELLER & CLASS Traveller (s) Economy Defence Forces Students Senior Citizens Doctors & Nurses Home Flights Bangalore To Hyderabad Flights January ₹12807 February ₹2972 March ₹2657 April ₹2775 May ₹2775 June ₹2775 July ₹2775 August'}]\u001b[0m\u001b[32;1m\u001b[1;3m We need to select the appropriate flight cost from the search results.\n",
      "Action: calculator\n",
      "Action Input: \"12807 * 2\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "12807 * 2\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: 12807 * 2\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```text\n",
      "12807 * 2\n",
      "```\n",
      "...numexpr.evaluate(\"12807 * 2\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m25614\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{'question': '12807 * 2', 'answer': 'Answer: 25614'}\u001b[0m\u001b[32;1m\u001b[1;3m We need to double check to make sure we calculated correctly.\n",
      "Action: calculator\n",
      "Action Input: \"25614 / 2\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "25614 / 2\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: 25614 / 2\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```text\n",
      "25614 / 2\n",
      "```\n",
      "...numexpr.evaluate(\"25614 / 2\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m12807.0\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{'question': '25614 / 2', 'answer': 'Answer: 12807.0'}\u001b[0m\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The flight cost from Bangalore to Hyderabad is 12807 rupees and when multiplied by 2, the total cost would be 25614 rupees.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How much does a flight cost from Bangalore to Hyderabad? Multiply?',\n",
       " 'output': 'The flight cost from Bangalore to Hyderabad is 12807 rupees and when multiplied by 2, the total cost would be 25614 rupees.'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"How much does a flight cost from Bangalore to Hyderabad? Multiply?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd56d2a-eb33-4249-b64d-1f6d1c01824b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

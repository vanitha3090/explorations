{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (Prompting Techniques / RAG ):\n",
    "Dataset: train_40k.csv\n",
    "\n",
    "Design and implement a classifier using any LLM to classify the data in Column name “Text” with Column name “Cat2” and Column name “Cat3”.\n",
    "2.1 - Input to your prompt will be a text from column name Text, and output should be class name from Column Name Cat 2\n",
    "2.2 - 2.1 - Input to your prompt will be a text from column name Text, and output should be class name from Column Name Cat 3\n",
    "\n",
    "Report the Accuracy on a sample test set split from 40k samples.\n",
    "\n",
    "Submission: Task2.ipynb notebook file and a readme file explaining the approach to the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches:\n",
    "Large Language Models (LLMs) have revolutionized the classification process by offering real-time text data categorization, perfect for applications requiring swift decision-making.\n",
    "\n",
    "- Prompt Base Approach: By providing an appropriate prompt alongside item descriptions and a list of item classes, LLMs efficiently categorize items. This approach works best when the prompt fits within the model's context window.\n",
    "\n",
    "- Retrieval Augmented Generation (RAG): RAG is invaluable when item classes exceed the context window. Here, item classes are converted into embeddings and stored in a Vector DB or Open Search with an ANN Plugin. LLMs, when prompted without the item class list, reference the Vector DB to retrieve top relevant item classes. These are then used to classify the item description.\n",
    "\n",
    "- Fine Tuning: With sufficient training data, fine-tuning LLMs using a specialized dataset containing item descriptions and their classes enhances classification performance. This process merges pre-trained model knowledge with task-specific data, improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"train_40k.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>Title</th>\n",
       "      <th>userId</th>\n",
       "      <th>Helpfulness</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cat1</th>\n",
       "      <th>Cat2</th>\n",
       "      <th>Cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000E46LYG</td>\n",
       "      <td>Golden Valley Natural Buffalo Jerky</td>\n",
       "      <td>A3MQDNGHDJU4MK</td>\n",
       "      <td>0/0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>The description and photo on this product need...</td>\n",
       "      <td>grocery gourmet food</td>\n",
       "      <td>meat poultry</td>\n",
       "      <td>jerky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000GRA6N8</td>\n",
       "      <td>Westing Game</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>860630400</td>\n",
       "      <td>This was a great book!!!! It is well thought t...</td>\n",
       "      <td>toys games</td>\n",
       "      <td>games</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000GRA6N8</td>\n",
       "      <td>Westing Game</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>883008000</td>\n",
       "      <td>I am a first year teacher, teaching 5th grade....</td>\n",
       "      <td>toys games</td>\n",
       "      <td>games</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000GRA6N8</td>\n",
       "      <td>Westing Game</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>897696000</td>\n",
       "      <td>I got the book at my bookfair at school lookin...</td>\n",
       "      <td>toys games</td>\n",
       "      <td>games</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00000DMDQ</td>\n",
       "      <td>I SPY A is For Jigsaw Puzzle 63pc</td>\n",
       "      <td>unknown</td>\n",
       "      <td>4-Feb</td>\n",
       "      <td>5</td>\n",
       "      <td>911865600</td>\n",
       "      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n",
       "      <td>toys games</td>\n",
       "      <td>puzzles</td>\n",
       "      <td>jigsaw puzzles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productId                                Title          userId  \\\n",
       "0  B000E46LYG  Golden Valley Natural Buffalo Jerky  A3MQDNGHDJU4MK   \n",
       "1  B000GRA6N8                         Westing Game         unknown   \n",
       "2  B000GRA6N8                         Westing Game         unknown   \n",
       "3  B000GRA6N8                         Westing Game         unknown   \n",
       "4  B00000DMDQ    I SPY A is For Jigsaw Puzzle 63pc         unknown   \n",
       "\n",
       "  Helpfulness  Score       Time  \\\n",
       "0         0/0      3         -1   \n",
       "1         0/0      5  860630400   \n",
       "2         0/0      5  883008000   \n",
       "3         0/0      5  897696000   \n",
       "4       4-Feb      5  911865600   \n",
       "\n",
       "                                                Text                  Cat1  \\\n",
       "0  The description and photo on this product need...  grocery gourmet food   \n",
       "1  This was a great book!!!! It is well thought t...            toys games   \n",
       "2  I am a first year teacher, teaching 5th grade....            toys games   \n",
       "3  I got the book at my bookfair at school lookin...            toys games   \n",
       "4  Hi! I'm Martine Redman and I created this puzz...            toys games   \n",
       "\n",
       "           Cat2            Cat3  \n",
       "0  meat poultry           jerky  \n",
       "1         games         unknown  \n",
       "2         games         unknown  \n",
       "3         games         unknown  \n",
       "4       puzzles  jigsaw puzzles  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the task heavily depends on only three columns (Text, Cat2, Cat3) and dropping other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"productId\",\n",
    "    \"Title\",\n",
    "    \"userId\",\n",
    "    \"Helpfulness\",\n",
    "    \"Score\",\n",
    "    \"Time\",\n",
    "    \"Cat1\",\n",
    "]  # List of columns to drop\n",
    "data = data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text          Cat2  \\\n",
      "0  The description and photo on this product need...  meat poultry   \n",
      "1  This was a great book!!!! It is well thought t...         games   \n",
      "2  I am a first year teacher, teaching 5th grade....         games   \n",
      "3  I got the book at my bookfair at school lookin...         games   \n",
      "4  Hi! I'm Martine Redman and I created this puzz...       puzzles   \n",
      "\n",
      "             Cat3  \n",
      "0           jerky  \n",
      "1         unknown  \n",
      "2         unknown  \n",
      "3         unknown  \n",
      "4  jigsaw puzzles  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meat poultry' 'games' 'puzzles' 'beverages' 'makeup' 'arts crafts'\n",
      " 'action toy figures' 'dolls accessories' 'baby toddler toys'\n",
      " 'personal care' 'nutrition wellness' 'learning education'\n",
      " 'electronics for kids' 'household supplies' 'stuffed animals plush'\n",
      " 'tricycles' 'health care' 'gear' 'skin care' 'grown up toys'\n",
      " 'dress up pretend play' 'novelty gag toys' 'bath body'\n",
      " 'tools accessories' 'hair care' 'medical supplies equipment'\n",
      " 'baby child care' 'building toys' 'gifts' 'sexual wellness'\n",
      " 'sports outdoor play' 'hobbies' 'feeding' 'diapering' 'safety' 'nursery'\n",
      " 'bathing skin care' 'vehicles remote control' 'car seats accessories'\n",
      " 'strollers' 'pregnancy maternity' 'cats' 'potty training' 'dogs'\n",
      " 'gourmet gifts' 'sauces dips' 'breakfast foods' 'pantry staples'\n",
      " 'fragrance' 'fresh flowers live indoor plants' 'breads bakery'\n",
      " 'candy chocolate' 'cooking baking supplies' 'snack food' 'meat seafood'\n",
      " 'herbs' 'baby food' 'fish aquatic pets' 'small animals' 'dairy eggs'\n",
      " 'birds' 'produce' 'health baby care' 'bunny rabbit central']\n",
      "64\n",
      "['jerky' 'unknown' 'jigsaw puzzles' 'board games' 'juices' 'nails'\n",
      " 'drawing painting supplies' 'figures' 'dolls' 'card games'\n",
      " 'drawing sketching tablets' 'shape sorters' 'deodorants antiperspirants'\n",
      " 'nutrition bars drinks' 'habitats' 'household batteries' 'push pull toys'\n",
      " 'scooters wagons' 'clay dough' 'allergy' 'baby gyms playmats'\n",
      " 'shaving hair removal' 'face' 'animals figures' 'feminine care'\n",
      " 'music sound' 'oral hygiene' 'pretend play' 'cleansers' 'playsets'\n",
      " 'd puzzles' 'dollhouses' 'lip care products' 'nail tools' 'eye care'\n",
      " 'pill cases splitters' 'styling products' 'electronic toys' 'body'\n",
      " 'toy balls' 'eyes' 'trading card games' 'foot care' 'hands nails' 'sun'\n",
      " 'daily living aids' 'personal care' 'paper plastic' 'incontinence'\n",
      " 'shampoos' 'conditioners' 'music players karaoke' 'cough cold' 'bath'\n",
      " 'tests' 'building sets' 'stress reduction'\n",
      " 'family planning contraceptives' 'vitamins supplements' 'hair color'\n",
      " 'pain relievers' 'cotton swabs' 'styling tools' 'first aid'\n",
      " 'scrubs body treatments' 'cleaning tools' 'pegged puzzles' 'diabetes'\n",
      " 'magic kits accessories' 'albums' 'crib toys attachments'\n",
      " 'digestion nausea' 'electronic pets' 'safer sex' 'thermometers'\n",
      " 'stacking nesting toys' 'makeup remover' 'temporary tattoos'\n",
      " 'play tents tunnels' 'science' 'sports' 'bath toys' 'puppets'\n",
      " 'systems accessories' 'action toy figures' 'health monitors'\n",
      " 'inflatable bouncers' 'model building kits tools'\n",
      " 'blackboards whiteboards' 'pools water fun' 'rattles'\n",
      " 'sandboxes accessories' 'activity play centers' 'car seat stroller toys'\n",
      " 'bottle feeding' 'breastfeeding' 'diaper changing kits'\n",
      " 'puzzle accessories' 'diaper pails refills' 'bathroom safety'\n",
      " 'massage relaxation' 'gates doorways' 'furniture' 'monitors'\n",
      " 'cloth diapers' 'plush backpacks purses' 'statues' 'bathing tubs seats'\n",
      " 'play vehicles' 'backpacks carriers' 'craft kits' 'car seats'\n",
      " 'nursery d cor' 'hammering pounding toys' 'bedding'\n",
      " 'play trains railway sets' 'rockets' 'stacking blocks' 'diaper bags'\n",
      " 'gym sets swings' 'maternity pillows' 'rocking spring ride ons' 'braces'\n",
      " 'accessories' 'vehicle playsets' 'doll accessories'\n",
      " 'litter housebreaking' 'spinning tops' 'sets' 'travel games'\n",
      " 'pillows stools' 'battling tops' 'cameras camcorders' 'dance mats' 'food'\n",
      " 'radio control' 'grooming healthcare kits' 'balls' 'tile games'\n",
      " 'potties seats' 'highchairs booster seats' 'stuffed animals toys'\n",
      " 'dvd games' 'edge corner guards' 'basic life skills toys'\n",
      " 'activity centers entertainers' 'thermometer accessories' 'wipes holders'\n",
      " 'gift sets' 'solid feeding' 'joggers' 'health care' 'facial steamers'\n",
      " 'kites wind spinners' 'toys' 'walkers' 'slumber bags' 'die cast vehicles'\n",
      " 'easels' 'lips' 'tea' 'reading writing' 'snack gifts' 'stacking games'\n",
      " 'sauces' 'cereals' 'shopping cart covers' 'scaled model vehicles'\n",
      " 'cooking baking supplies' 'personal video players accessories' 'women s'\n",
      " 'keepsakes' 'treats' 'swings' 'trains accessories' 'disposable diapers'\n",
      " 'plug play video games' 'floor puzzles' 'live indoor plants'\n",
      " 'weight loss products' 'smoking cessation' 'beauty fashion' 'mirrors'\n",
      " 'coffee' 'cabinet locks straps' 'plush pillows' 'floor games'\n",
      " 'makeup brushes tools' 'alternative medicine'\n",
      " 'changing table pads covers' 'men s' 'step stools' 'rails rail guards'\n",
      " 'laundry' 'women s health' 'standard' 'beds furniture' 'herbs'\n",
      " 'sleep positioners' 'health supplies' 'breakfast cereal bars' 'body art'\n",
      " 'condiments' 'cakes' 'dried beans' 'household cleaning' 'collars'\n",
      " 'educational repellents' 'dessert gifts' 'adult toys games' 'teddy bears'\n",
      " 'therapeutic skin care' 'chocolate assortments' 'sand water tables'\n",
      " 'slot cars' 'chocolate' 'soft drinks' 'chips crisps' 'licorice'\n",
      " 'feeding watering supplies' 'blasters foam play' 'skin care'\n",
      " 'chocolate bars' 'wild game fowl' 'spices seasonings' 'cookies'\n",
      " 'training behavior aids' 'gardening tools' 'bathroom aids safety'\n",
      " 'pogo sticks hoppers' 'powdered drink mixes' 'playards'\n",
      " 'gag toys practical jokes' 'baby formula' 'lighters'\n",
      " 'dried fruit raisins' 'money banks' 'marble runs' 'game collections'\n",
      " 'kitchen safety' 'gum' 'outdoor safety' 'hair nails' 'aquarium lights'\n",
      " 'blocks' 'tandem' 'occupational physical therapy aids'\n",
      " 'packaged meals side dishes' 'indoor climbers play structures'\n",
      " 'pumps filters' 'beds accessories' 'energy drinks' 'sleep snoring'\n",
      " 'geography' 'houses habitats' 'cheese' 'travel systems' 'walkie talkies'\n",
      " 'mobility aids equipment' 'sexual enhancers' 'dips'\n",
      " 'dollhouse accessories' 'party mix' 'bathing accessories' 'grooming'\n",
      " 'baby seats' 'wind up toys' 'sensual delights' 'hot cocoa' 'dishwashing'\n",
      " 'carriers strollers' 'flash cards' 'novelty gag toys' 'brain teasers'\n",
      " 'nesting dolls' 'test kits' 'lightweight' 'hair loss products'\n",
      " 'water treatments' 'hair scalp treatments' 'cages accessories'\n",
      " 'gummy candies' 'gummy candy' 'jerky dried meats' 'houses'\n",
      " 'chocolate truffles' 'ear care' 'milk' 'pizza crusts'\n",
      " 'fresh baked cookies' 'hard candies' 'sports supplements' 'baking mixes'\n",
      " 'crackers' 'pork rinds' 'pasta noodles' 'trail mix'\n",
      " 'carriers travel products' 'fresh fruits' 'toaster pastries' 'rice cakes'\n",
      " 'chips' 'puffed snacks' 'pretzels' 'mathematics counting'\n",
      " 'suckers lollipops' 'popcorn' 'toy banks' 'training pants' 'tea gifts'\n",
      " 'oils' 'aquarium hoods' 'tortillas' 'cheese gifts' 'doors'\n",
      " 'standard playing card decks' 'fudge' 'syrups' 'printing stamping'\n",
      " 'toy gift sets' 'canned jarred food' 'fresh vegetables'\n",
      " 'apparel accessories' 'candy gifts' 'chewing gum' 'puzzle play mats'\n",
      " 'electrical safety' 'sugars sweeteners' 'marble games' 'miniatures'\n",
      " 'finger boards finger bikes' 'coconut water' 'handheld games'\n",
      " 'slime putty toys' 'pastries' 'teethers' 'butter' 'fruits'\n",
      " 'chocolate pretzels' 'breakfast bakery' 'stickers' 'soaps cleansers'\n",
      " 'sauces gifts' 'hobbies' 'fitness equipment' 'water'\n",
      " 'portable changing pads' 'dice gaming dice' 'pacifiers accessories'\n",
      " 'bars' 'cocktail mixers' 'aquariums' 'ball pits accessories' 'seafood'\n",
      " 'bags cases' 'jelly beans' 'novelty spinning tops' 'automatic feeders'\n",
      " 'mints' 'makeup sets' 'cleaners' 'fresh cut flowers' 'jams' 'prams'\n",
      " 'nuts seeds' 'taffy' 'rabbit hutches' 'aquarium d cor' 'granola bars'\n",
      " 'viewfinders' 'harnesses leashes' 'puzzles' 'foie gras p t s'\n",
      " 'game accessories' 'p t s' 'game room games' 'cages' 'non slip bath mats'\n",
      " 'halva' 'games' 'seafood gifts' 'stimulants' 'beanbags foot bags'\n",
      " 'shampoo conditioner sets' 'nut clusters' 'raisins' 'breadcrumbs'\n",
      " 'extracts flavoring' 'plush puppets' 'shampoo plus conditioner' 'salsas'\n",
      " 'memorials' 'die cast toy vehicles' 'aquarium starter kits'\n",
      " 'coffee gifts' 'air fresheners' 'fruit leather' 'granola trail mix bars'\n",
      " 'sugar substitutes' 'bacon' 'cat flaps' 'aquarium heaters'\n",
      " 'hair relaxers' 'breads' 'packaged breads' 'sausages' 'dessert toppings'\n",
      " 'diaper stackers caddies' 'prisms kaleidoscopes' 'maternity'\n",
      " 'crackers biscuits' 'coin collecting' 'chocolate gifts'\n",
      " 'kickball playground balls' 'hair perms texturizers' 'yo yos'\n",
      " 'flours meals' 'beef' 'molding sculpting sticks' 'washcloths towels'\n",
      " 'fruit gifts' 'stuffing' 'baking powder' 'cereal' 'exotic meats'\n",
      " 'breadsticks' 'eggs' 'cloth diaper accessories' 'carriers' 'toffee'\n",
      " 'hair coloring tools' 'chocolate covered fruit' 'caramels' 'assortments'\n",
      " 'aromatherapy' 'seat covers' 'bondage gear accessories' 'sun protection'\n",
      " 'dinners' 'fruit' 'aquarium stands' 'teaching clocks' 'milk substitutes'\n",
      " 'bubble bath' 'novelties' 'jams preserves gifts' 'meat gifts' 'beads'\n",
      " 'fish bowls' 'odor stain removers' 'beverages' 'food coloring'\n",
      " 'children s' 'ice cream frozen desserts' 'pastry decorations' 'chicken'\n",
      " 'sports drinks' 'marshmallows' 'pudding' 'aprons smocks' 'electronics'\n",
      " 'sex furniture' 'pork' 'spices gifts']\n",
      "464\n"
     ]
    }
   ],
   "source": [
    "category2_classes = data[\"Cat2\"].unique()\n",
    "print(category2_classes)\n",
    "print(len(category2_classes))\n",
    "\n",
    "category3_classes = data[\"Cat3\"].unique()\n",
    "print(category3_classes)\n",
    "print(len(category3_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data set to train & test using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the test data into another CSV file\n",
    "test_df.to_csv(\"test_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prompt Base Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from openai==0.28) (3.8.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from openai==0.28) (4.64.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from openai==0.28) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai==0.28) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\vanitha.alagarsamy\\anaconda3\\lib\\site-packages (from tqdm->openai==0.28) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_generation(question):\n",
    "    temperature = 0.0\n",
    "    max_tokens = 64\n",
    "    top_p = 0.9\n",
    "    best_of = 1\n",
    "    frequency_penalty = 0.0\n",
    "    presence_penalty = 0.0\n",
    "    stop = ['===']\n",
    "    output = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        stop=stop,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p, )\n",
    "    return output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prompting\n",
    "Defining the Prompt and adding the constraints in the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a classification engine. Your task is to classify the text from the given list of categories - {categories}.\n",
    "You must not classify anything out the list of categories provided. Return unknown if the text cannot be classifed\n",
    "Now classify for this text:\n",
    "User Query: {sentence}\n",
    "\"\"\"\n",
    "\n",
    "# prompt = \"\"\"You are a classification engine. Your task is to classify the text from the given list of categories - {categories}.\n",
    "# You must not classify anything out the list of categories provided. Return unknown if the text cannot be classifed\n",
    "# Text:My wife has been wonderfully pleased with this product. It leaves her cockapoo soft and shiny. She says that it the best she has ever used.\n",
    "# Class:dogs\n",
    "# Text:{sentence}\n",
    "# Class:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a classification engine. Your task is to classify the text from the given list of categories - {categories}.\n",
      "You must not classify anything out the list of categories provided. Return unknown if the text cannot be classifed\n",
      "Now classify for this text:\n",
      "User Query: {sentence}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"<add your openai key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Category 2 Classification using GPT4 model for all the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "test_shuffled_data = test_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data['Text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'household supplies'\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prompt=prompt.format(categories=category2_classes,sentence=test_data['Text'][0])\n",
    "gpt3_output = chatgpt_generation(sample_prompt)\n",
    "gpt3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_prompt = \"{A} \\nExplain the above in one sentence:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m each_text \u001b[38;5;129;01min\u001b[39;00m test_shuffled_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#summ_each_prompt = summ_prompt.format(A=test_shuffled_data['Text'][index])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     summ_each_prompt \u001b[38;5;241m=\u001b[39m summ_prompt\u001b[38;5;241m.\u001b[39mformat(A\u001b[38;5;241m=\u001b[39meach_text)\n\u001b[1;32m----> 7\u001b[0m     summ_output \u001b[38;5;241m=\u001b[39m \u001b[43mchatgpt_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msumm_each_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     each_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(categories\u001b[38;5;241m=\u001b[39mcategory2_classes,sentence\u001b[38;5;241m=\u001b[39msumm_output)\n\u001b[0;32m      9\u001b[0m     gpt3_output \u001b[38;5;241m=\u001b[39m chatgpt_generation(each_prompt)\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mchatgpt_generation\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m      7\u001b[0m presence_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      8\u001b[0m stop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m===\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "cat2_output = []\n",
    "# Iterate over the first 100 entries of the DataFrame\n",
    "#for index, row in test_shuffled_data.head(5).iterrows():\n",
    "for each_text in test_shuffled_data['Text']:\n",
    "    #summ_each_prompt = summ_prompt.format(A=test_shuffled_data['Text'][index])\n",
    "    summ_each_prompt = summ_prompt.format(A=each_text)\n",
    "    summ_output = chatgpt_generation(summ_each_prompt)\n",
    "    each_prompt = prompt.format(categories=category2_classes,sentence=summ_output)\n",
    "    gpt3_output = chatgpt_generation(each_prompt)\n",
    "    cat2_output.append(gpt3_output)\n",
    "#     print(\"\\n\")\n",
    "#     print(test_data['Text'][index])\n",
    "#     print(\"***\")\n",
    "#     print(summ_output,gpt3_output)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "    \n",
    "# for each_text in test_data['Text']:\n",
    "#     cat2_output.append (chatgpt_generation(prompt+each_text) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3315\n"
     ]
    }
   ],
   "source": [
    "print(len(cat2_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The text can be classified under 'personal care'.\", \"'novelty gag toys'\", \"'games'\", \"'nutrition wellness'\", \"'beverages'\"]\n"
     ]
    }
   ],
   "source": [
    "print(cat2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_prompt = \"{A} \\nExplain the above in one sentence:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Category3 Classification using GPT4 model for all the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat3_output = []\n",
    "#for index, row in test_shuffled_data.head(5).iterrows():\n",
    "for each_text in test_shuffled_data['Text']:\n",
    "    #summ_each_prompt = summ_prompt.format(A=test_shuffled_data['Text'][index])\n",
    "    summ_each_prompt = summ_prompt.format(A=each_text)\n",
    "    summ_output = chatgpt_generation(summ_each_prompt)\n",
    "    each_prompt = prompt.format(categories=category3_classes,sentence=summ_output)\n",
    "    gpt3_output = chatgpt_generation(each_prompt)\n",
    "    cat3_output.append(gpt3_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing back the predictions to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predictions to a new column in the DataFrame\n",
    "test_data['Cat2_Predictions'] = cat2_output\n",
    "test_data['Cat3_Predictions'] = cat3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame back to the CSV file\n",
    "test_data.to_csv('test_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy for the LLM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(expected_values,predicted_values):\n",
    "    # Calculate accuracy\n",
    "    total_samples = len(expected_values)\n",
    "    print(len(expected_values))\n",
    "    correct_predictions = sum(1 for exp, pred in zip(expected_values, predicted_values) if exp == pred)\n",
    "    accuracy = (correct_predictions / total_samples) * 100\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2_output = [item.replace(\"'\", \"\") for item in cat2_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3315\n",
      "Accuracy: 51.25%\n",
      "Cat2 accuracy 51.251885369532424\n"
     ]
    }
   ],
   "source": [
    "# accuracy for Cat2\n",
    "cat2_expected_values = test_shuffled_data['Cat2'].head(3315).tolist()\n",
    "\n",
    "print(\"Cat2 accuracy\",calculate_accuracy(cat2_expected_values,cat2_output))\n",
    "\n",
    "# # accuracy for Cat3\n",
    "# cat3_expected_values = test_data['Cat3'].tolist() \n",
    "# print(calculate_accuracy(cat3_expected_values,cat3_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy: 51.25%**\n",
    "\n",
    "**Cat2 accuracy 51.251885369532424**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({'Text':test_shuffled_data['Text'].head(3315).tolist(),'Expected':cat2_expected_values,'Precited':cat2_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text  \\\n",
      "0     I'm really happy with this soap. I love that i...   \n",
      "1     My son has wanted to learn to do magic tricks,...   \n",
      "2     i just bought this for my daughters 3rd bday a...   \n",
      "3     Ive heard many good things about biotin and I ...   \n",
      "4     Basically, this is a powder mix for one of tho...   \n",
      "...                                                 ...   \n",
      "3310  Sweet lily of the valley main note with a bit ...   \n",
      "3311  Ordered the Sandalwood Vanilla, I received it ...   \n",
      "3312  My birds so happy to be able to hop from the m...   \n",
      "3313  THE FIRST TIME I SAW THIS GATE I THOUGHT IT WA...   \n",
      "3314  Along with Hokey Pokey Elmo, Chicken Dance Elm...   \n",
      "\n",
      "                   Expected            Precited  \n",
      "0                 bath body       personal care  \n",
      "1          novelty gag toys    novelty gag toys  \n",
      "2      electronics for kids               games  \n",
      "3        nutrition wellness  nutrition wellness  \n",
      "4        nutrition wellness           beverages  \n",
      "...                     ...                 ...  \n",
      "3310              fragrance           fragrance  \n",
      "3311              fragrance           fragrance  \n",
      "3312                  birds               birds  \n",
      "3313                 safety              safety  \n",
      "3314  stuffed animals plush  action toy figures  \n",
      "\n",
      "[3315 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Tasks:\n",
    "- As GPT-4 model has best performance, i used that model for our experiment\n",
    "- As the quota of my Openai key is over, could able to perform only 3315 entries for Category 2 and not able to perform for Category 3 classification\n",
    "- We can try using other open-source models like Llama, Openchat, and Mistral, but there may be a trade-off in accuracy.\n",
    "\n",
    "#### Future improvements:\n",
    "When it comes to improving the accuracy of zero-shot prompting for multi-label and multi-class classification using GPT-4, there are several approaches and future directions we can consider,\n",
    "\n",
    "- Retrieval Augmented Generation (RAG): RAG is invaluable when item classes exceed the context window. Here, item classes are converted into embeddings and stored in a Vector DB or Open Search with an ANN Plugin. LLMs, when prompted without the item class list, reference the Vector DB to retrieve top relevant item classes. These are then used to classify the item description.\n",
    "\n",
    "- Fine Tuning: With sufficient training data, fine-tuning LLMs using a specialized dataset containing item descriptions and their classes enhances classification performance. This process merges pre-trained model knowledge with task-specific data, improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
